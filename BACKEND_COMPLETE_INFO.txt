================================================================================
          CHEPPU AI - COMPLETE BACKEND INFORMATION & DOCUMENTATION
================================================================================

Generated: November 24, 2025
Backend URL: https://cheppu-aichatbox.onrender.com
Platform: Render.com (Free Tier)
Runtime: Node.js 14+
Status: Production Live

================================================================================
                            TABLE OF CONTENTS
================================================================================

1. BACKEND OVERVIEW
2. SERVER CONFIGURATION
3. COMPLETE SERVER CODE (server.js)
4. ALTERNATIVE PROXY CODE (proxy-server.js)
5. DEPENDENCIES (package.json)
6. ENVIRONMENT VARIABLES
7. API ENDPOINTS DOCUMENTATION
8. REQUEST/RESPONSE EXAMPLES
9. ERROR HANDLING
10. DEPLOYMENT INFORMATION
11. MONITORING & LOGS
12. TROUBLESHOOTING GUIDE

================================================================================
                         1. BACKEND OVERVIEW
================================================================================

PURPOSE:
The backend serves as a PROXY SERVER to handle API requests from the frontend
and forward them to various AI service providers while keeping API keys secure.

KEY RESPONSIBILITIES:
- Proxy chat requests to Groq API
- Proxy image/TTS requests to HuggingFace
- Handle CORS (Cross-Origin Resource Sharing)
- Implement model fallback logic
- Hide API keys from client-side code
- Stream binary responses (images, audio)
- Error handling and logging

ARCHITECTURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend   â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚    Proxy     â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚   AI APIs    â”‚
â”‚  (Netlify)   â”‚         â”‚  (Render)    â”‚         â”‚ Groq/HF/etc  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DEPLOYMENT:
Platform: Render.com
URL: https://cheppu-aichatbox.onrender.com
Region: US-West
Instance: Free Tier (auto-sleep after 15min inactivity)
Auto-Deploy: Enabled (on git push to main)

================================================================================
                        2. SERVER CONFIGURATION
================================================================================

HOSTING PLATFORM: Render.com
-------------------------
Service Name: cheppu-aichatbox
Service Type: Web Service
Plan: Free Tier
Region: Oregon (US-West)

BUILD SETTINGS:
Build Command: npm install
Start Command: node server.js
Root Directory: /
Auto-Deploy: Yes (from main branch)

ENVIRONMENT VARIABLES:
Key                  Description                      Required
---                  -----------                      --------
HF_TOKEN             HuggingFace API Token           Yes
GROQ_API_KEY         Groq API Key                    Yes
PORT                 Server Port (auto by Render)    No
NODE_ENV             Environment (production)         No

INSTANCE SPECS:
RAM: 512 MB (free tier)
CPU: Shared
Disk: 1 GB
Bandwidth: 100 GB/month

NETWORKING:
Protocol: HTTPS only
SSL: Automatic (Let's Encrypt)
IPv4: Enabled
IPv6: Enabled

AUTO-SCALING:
Free tier: Single instance
Auto-sleep: After 15 minutes of inactivity
Wake-up: First request after sleep (~30 seconds)

================================================================================
                    3. COMPLETE SERVER CODE (server.js)
================================================================================

FILENAME: server.js
SIZE: 8,456 bytes (215 lines)
LANGUAGE: JavaScript (Node.js)
DEPENDENCIES: express, cors, node-fetch

--------------------------------------------------------------------------------
                           FULL CODE LISTING
--------------------------------------------------------------------------------

// Production-ready Express server for HuggingFace API proxy
const express = require('express');
const cors = require('cors');
const fetch = require('node-fetch');

const app = express();
const PORT = process.env.PORT || 3000;
const API_KEY = process.env.HF_TOKEN || "YOUR_HUGGINGFACE_TOKEN_HERE";
const GROQ_API_KEY = process.env.GROQ_API_KEY || "YOUR_GROQ_API_KEY_HERE";

// Log startup info (token masked for security)
console.log('Server starting...');
console.log('PORT:', PORT);
console.log('HF_TOKEN present:', !!process.env.HF_TOKEN);
console.log('API_KEY starts with:', API_KEY ? API_KEY.substring(0, 6) + '...' : 'NOT SET');

// Middleware
app.use(cors());
app.use(express.json({ limit: '50mb' }));

// Health check endpoint
app.get('/', (req, res) => {
    res.json({
        status: 'running',
        message: 'Cheppu AI Chatbot Proxy Server',
        endpoints: {
            proxy: 'POST /',
            health: 'GET /health'
        }
    });
});

app.get('/health', (req, res) => {
    res.json({ status: 'healthy', timestamp: new Date().toISOString() });
});

// Main proxy endpoint
app.post('/', async (req, res) => {
    try {
        const { model, inputs, type, messages } = req.body;

        // Handle chat requests
        if (type === 'chat' || messages) {
            // Priority list of models for fallback - comprehensive list from user requirements
            const modelPriority = [
                model, // Try requested model first
                "llama-3.1-8b-instant",
                "llama-3.3-70b-versatile",
                "qwen/qwen3-32b",
                "groq/compound",
                "groq/compound-mini",
                "meta-llama/llama-4-maverick-17b-128e-instruct",
                "meta-llama/llama-4-scout-17b-16e-instruct",
                "moonshotai/kimi-k2-instruct",
                "moonshotai/kimi-k2-instruct-0905",
                "openai/gpt-oss-120b",
                "openai/gpt-oss-20b",
                "openai/gpt-oss-safeguard-20b",
                // Additional reliable fallbacks
                "gemma2-9b-it",
                "mixtral-8x7b-32768"
            ].filter(Boolean); // Remove null/undefined

            // Remove duplicates
            const uniqueModels = [...new Set(modelPriority)];
            
            console.log(`[${new Date().toISOString()}] Chat request. Attempting models: ${uniqueModels.join(', ')}`);

            let lastError = null;
            const chatUrl = 'https://api.groq.com/openai/v1/chat/completions';

            for (const currentModel of uniqueModels) {
                try {
                    console.log(`Trying model: ${currentModel}...`);
                    
                    const response = await fetch(chatUrl, {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${GROQ_API_KEY}`,
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({
                            model: currentModel,
                            messages: messages,
                            temperature: 0.7,
                            max_tokens: 4096
                        })
                    });

                    if (!response.ok) {
                        const errorText = await response.text();
                        console.warn(`Model ${currentModel} failed: ${response.status} - ${errorText}`);
                        
                        // If it's an auth error, don't retry other models (key is invalid)
                        if (response.status === 401) {
                            throw new Error('Invalid API Key');
                        }
                        
                        lastError = new Error(`Model ${currentModel} failed: ${response.status}`);
                        continue; // Try next model
                    }

                    const result = await response.json();
                    console.log(`Success with model: ${currentModel}`);
                    
                    // Add metadata about which model was actually used
                    result.used_model = currentModel;
                    return res.json(result);

                } catch (err) {
                    console.error(`Error with model ${currentModel}:`, err.message);
                    lastError = err;
                    // Continue to next model
                }
            }

            // If we get here, all models failed
            console.error('All models failed');
            return res.status(500).json({
                error: 'All AI models are currently unavailable. Please try again later.',
                details: lastError ? lastError.message : 'Unknown error'
            });
        }

        // Handle image and TTS requests
        if (!model || !inputs) {
            return res.status(400).json({ error: 'Missing required fields: model and inputs' });
        }

        console.log(`[${new Date().toISOString()}] ${type || 'unknown'} request for model: ${model}`);
        console.log(`Input length: ${inputs.length} characters`);

        // Make request to HuggingFace - NEW API ENDPOINT
        const hfUrl = `https://router.huggingface.co/hf-inference/models/${model}`;
        
        const response = await fetch(hfUrl, {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${API_KEY}`,
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({
                inputs: inputs,
                options: {
                    wait_for_model: true,
                    use_cache: false
                }
            })
        });

        if (!response.ok) {
            const errorText = await response.text();
            console.error(`HuggingFace API error: ${response.status}`, errorText);
            
            // Return the error with status code
            return res.status(response.status).json({
                error: `HuggingFace API error: ${response.status}`,
                details: errorText
            });
        }

        // Get content type from response
        const contentType = response.headers.get('content-type');
        
        // Stream the binary response
        const buffer = await response.buffer();
        
        // Set appropriate content type
        if (type === 'image') {
            res.setHeader('Content-Type', 'image/png');
        } else if (type === 'audio') {
            res.setHeader('Content-Type', 'audio/wav');
        } else {
            res.setHeader('Content-Type', contentType || 'application/octet-stream');
        }

        res.send(buffer);
        console.log(`Request completed successfully (${buffer.length} bytes)`);

    } catch (error) {
        console.error('Server error:', error);
        res.status(500).json({
            error: 'Internal server error',
            message: error.message
        });
    }
});

// 404 handler
app.use((req, res) => {
    res.status(404).json({ error: 'Not found' });
});

// Start server
app.listen(PORT, () => {
    console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ðŸš€ Cheppu AI Proxy Server                           â•‘
â•‘                                                        â•‘
â•‘   Running on: http://localhost:${PORT}                    â•‘
â•‘   Environment: ${process.env.NODE_ENV || 'development'}                              â•‘
â•‘                                                        â•‘
â•‘   API Key: ${API_KEY.substring(0, 10)}...                        â•‘
â•‘                                                        â•‘
â•‘   Ready to handle Image & TTS requests!               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    `);
});

// Graceful shutdown
process.on('SIGTERM', () => {
    console.log('SIGTERM received, shutting down gracefully...');
    process.exit(0);
});

--------------------------------------------------------------------------------
                         CODE EXPLANATION
--------------------------------------------------------------------------------

LINES 1-9: Setup & Configuration
- Import dependencies (express, cors, node-fetch)
- Read environment variables (PORT, API keys)
- Masked logging for security

LINES 17-19: Middleware
- CORS: Allow all origins for API access
- Body parser: Support JSON up to 50MB (for large requests)

LINES 22-35: Health Check Endpoints
- GET /: Shows server status and available endpoints
- GET /health: Returns timestamp for monitoring

LINES 38-187: Main Proxy Logic
- POST /: Handles all proxy requests
- Chat requests â†’ Groq API with model fallback
- Image/TTS â†’ HuggingFace API
- Buffer streaming for binary data

LINES 72-115: Model Fallback System
- Tries requested model first
- Falls back through 15+ alternative models
- Stops on auth errors (invalid API key)
- Returns success on first working model

LINES 190-214: Server Lifecycle
- 404 handler for unknown routes
- Server startup with ASCII art
- Graceful shutdown handler (SIGTERM)

================================================================================
                4. ALTERNATIVE PROXY CODE (proxy-server.js)
================================================================================

FILENAME: proxy-server.js
SIZE: 4,303 bytes (114 lines)
PURPOSE: Lightweight HTTP proxy without Express dependency

--------------------------------------------------------------------------------
                           FULL CODE LISTING
--------------------------------------------------------------------------------

// Simple proxy server to bypass CORS restrictions
// Run this with: node proxy-server.js

const http = require('http');
const https = require('https');

const API_KEY = process.env.HF_TOKEN || "YOUR_HUGGINGFACE_TOKEN_HERE";
const PORT = 3000;

const server = http.createServer((req, res) => {
    // Enable CORS
    res.setHeader('Access-Control-Allow-Origin', '*');
    res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
    res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

    // Handle preflight
    if (req.method === 'OPTIONS') {
        res.writeHead(200);
        res.end();
        return;
    }

    if (req.method === 'POST') {
        let body = '';

        req.on('data', chunk => {
            body += chunk.toString();
        });

        req.on('end', () => {
            try {
                const requestData = JSON.parse(body);
                const { model, inputs, type } = requestData;

                console.log(`\n[${new Date().toISOString()}] ${type} request for model: ${model}`);
                console.log(`Input: ${inputs.substring(0, 50)}...`);

                // Make request to HuggingFace
                const options = {
                    hostname: 'api-inference.huggingface.co',
                    path: `/models/${model}`,
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${API_KEY}`,
                        'Content-Type': 'application/json',
                    }
                };

                const hfReq = https.request(options, (hfRes) => {
                    console.log(`HuggingFace response status: ${hfRes.statusCode}`);

                    // Set appropriate content type
                    if (type === 'image') {
                        res.setHeader('Content-Type', 'image/png');
                    } else if (type === 'audio') {
                        res.setHeader('Content-Type', 'audio/wav');
                    } else {
                        res.setHeader('Content-Type', hfRes.headers['content-type'] || 'application/octet-stream');
                    }

                    res.writeHead(hfRes.statusCode);

                    // Stream the response
                    hfRes.on('data', chunk => {
                        res.write(chunk);
                    });

                    hfRes.on('end', () => {
                        res.end();
                        console.log(`Request completed successfully`);
                    });
                });

                hfReq.on('error', (error) => {
                    console.error('HuggingFace API error:', error);
                    res.writeHead(500);
                    res.end(JSON.stringify({ error: error.message }));
                });

                // Send the request body
                hfReq.write(JSON.stringify({
                    inputs: inputs,
                    options: {
                        wait_for_model: true,
                        use_cache: false
                    }
                }));
                hfReq.end();

            } catch (error) {
                console.error('Parse error:', error);
                res.writeHead(400);
                res.end(JSON.stringify({ error: 'Invalid request' }));
            }
        });
    } else {
        res.writeHead(405);
        res.end('Method not allowed');
    }
});

server.listen(PORT, () => {
    console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ðŸš€ Proxy Server Running on http://localhost:${PORT}   â•‘
â•‘                                                        â•‘
â•‘   This server bypasses CORS restrictions for          â•‘
â•‘   HuggingFace API calls (Image & TTS)                 â•‘
â•‘                                                        â•‘
â•‘   Keep this terminal window open while using the app  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
`);
});

--------------------------------------------------------------------------------
                      COMPARISON: server.js vs proxy-server.js
--------------------------------------------------------------------------------

Feature             server.js         proxy-server.js
-------             ---------         ---------------
Framework           Express           Pure Node.js HTTP
Dependencies        3 packages        0 packages
CORS                cors package      Manual headers
Chat Support        âœ… Yes (Groq)      âŒ No
Model Fallback      âœ… Yes             âŒ No
Error Handling      âœ… Advanced        âš ï¸ Basic
Logging             âœ… Detailed        âš ï¸ Basic
Production Ready    âœ… Yes             âš ï¸ Dev only
File Size           8.5 KB            4.3 KB
Lines of Code       215               114

RECOMMENDATION: Use server.js for production

================================================================================
                      5. DEPENDENCIES (package.json)
================================================================================

FILENAME: package.json
SIZE: 657 bytes (30 lines)

--------------------------------------------------------------------------------
                           FULL CONFIGURATION
--------------------------------------------------------------------------------

{
  "name": "cheppu-ai-chatbot",
  "version": "1.0.0",
  "description": "AI chatbot with chat, image generation, and text-to-speech capabilities",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "node server.js",
    "validate-assets": "node ./scripts/validate_assets.js"
  },
  "keywords": [
    "ai",
    "chatbot",
    "huggingface",
    "image-generation",
    "text-to-speech",
    "proxy-server"
  ],
  "author": "Your Name",
  "license": "MIT",
  "engines": {
    "node": ">=14.0.0"
  },
  "dependencies": {
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "node-fetch": "^2.6.7"
  }
}

--------------------------------------------------------------------------------
                        DEPENDENCY DETAILS
--------------------------------------------------------------------------------

1. express (^4.18.2)
   Purpose: Web application framework
   Size: ~210 KB
   Features: Routing, middleware, request/response handling
   Documentation: https://expressjs.com/

2. cors (^2.8.5)
   Purpose: Enable Cross-Origin Resource Sharing
   Size: ~12 KB
   Features: CORS headers, preflight handling
   Documentation: https://github.com/expressjs/cors

3. node-fetch (^2.6.7)
   Purpose: HTTP client for making API requests
   Size: ~35 KB
   Features: Promise-based fetch API, stream support
   Documentation: https://github.com/node-fetch/node-fetch

Total Dependencies Size: ~257 KB (excluding Node.js runtime)

NPM Scripts:
- npm start: Start production server
- npm run dev: Start development server (same as start)
- npm run validate-assets: Validate project assets

Engine Requirements:
- Node.js: >= 14.0.0
- npm: Latest recommended

================================================================================
                      6. ENVIRONMENT VARIABLES
================================================================================

REQUIRED VARIABLES:
-------------------

Variable Name: HF_TOKEN
Description: HuggingFace API Token for accessing Inference API
Required: Yes
Format: String (starts with "hf_")
Example: hf_abcdefghijklmnopqrstuvwxyz1234567890
Where to Get: https://huggingface.co/settings/tokens
Permissions: Read access
Used For: Image generation, TTS requests

Variable Name: GROQ_API_KEY
Description: Groq API Key for accessing LLM models
Required: Yes
Format: String (starts with "gsk_")
Example: gsk_abcdefghijklmnopqrstuvwxyz1234567890
Where to Get: https://console.groq.com/keys
Used For: Chat completions, model inference

OPTIONAL VARIABLES:
-------------------

Variable Name: PORT
Description: Server port number
Required: No
Default: 3000 (local), auto-assigned on Render
Format: Integer
Example: 3000

Variable Name: NODE_ENV
Description: Environment identifier
Required: No
Default: development
Options: development, production, test
Example: production

--------------------------------------------------------------------------------
                    SETTING ENVIRONMENT VARIABLES
--------------------------------------------------------------------------------

ON RENDER.COM:
1. Go to your service dashboard
2. Click "Environment" tab
3. Click "Add Environment Variable"
4. Enter key and value
5. Click "Save Changes"
6. Service will auto-restart

ON LOCAL (DEVELOPMENT):
1. Create .env file in project root
2. Add variables:
   HF_TOKEN=your_token_here
   GROQ_API_KEY=your_key_here
   PORT=3000
3. Install dotenv: npm install dotenv
4. Add to server.js: require('dotenv').config();

ON VERCEL:
1. Project Settings â†’ Environment Variables
2. Add HF_TOKEN and GROQ_API_KEY
3. Select environments (Production, Preview, Development)
4. Save

ON RAILWAY:
1. Project â†’ Variables tab
2. Click "New Variable"
3. Add HF_TOKEN and GROQ_API_KEY
4. Deploy

SECURITY NOTES:
- Never commit .env to Git
- Use .gitignore to exclude .env
- Rotate keys if exposed
- Use separate keys for dev/prod

================================================================================
                    7. API ENDPOINTS DOCUMENTATION
================================================================================

BASE URL: https://cheppu-aichatbox.onrender.com

--------------------------------------------------------------------------------
ENDPOINT 1: Health Check
--------------------------------------------------------------------------------

URL: GET /
Purpose: Check server status and view available endpoints
Authentication: None
Rate Limit: None

Response (200 OK):
{
  "status": "running",
  "message": "Cheppu AI Chatbot Proxy Server",
  "endpoints": {
    "proxy": "POST /",
    "health": "GET /health"
  }
}

Example cURL:
curl https://cheppu-aichatbox.onrender.com/

--------------------------------------------------------------------------------
ENDPOINT 2: Detailed Health Check
--------------------------------------------------------------------------------

URL: GET /health
Purpose: Get server health status with timestamp
Authentication: None
Rate Limit: None

Response (200 OK):
{
  "status": "healthy",
  "timestamp": "2025-11-24T13:30:26.123Z"
}

Example cURL:
curl https://cheppu-aichatbox.onrender.com/health

--------------------------------------------------------------------------------
ENDPOINT 3: Chat Proxy
--------------------------------------------------------------------------------

URL: POST /
Purpose: Proxy chat requests to Groq API with model fallback
Authentication: Handled server-side
Rate Limit: 30 RPM (Groq free tier)

Request Body:
{
  "type": "chat",
  "model": "llama-3.1-8b-instant",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Hello, how are you?"
    }
  ]
}

Response (200 OK):
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1732451234,
  "model": "llama-3.1-8b-instant",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello! I'm doing well, thank you for asking! How can I assist you today?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 18,
    "total_tokens": 43
  },
  "used_model": "llama-3.1-8b-instant"
}

Error Response (500):
{
  "error": "All AI models are currently unavailable. Please try again later.",
  "details": "Model llama-3.1-8b-instant failed: 503"
}

Example cURL:
curl -X POST https://cheppu-aichatbox.onrender.com/ \
  -H "Content-Type: application/json" \
  -d '{
    "type": "chat",
    "model": "llama-3.1-8b-instant",
    "messages": [
      {"role": "user", "content": "Hello"}
    ]
  }'

Supported Models:
- llama-3.1-8b-instant
- llama-3.3-70b-versatile
- qwen/qwen3-32b
- groq/compound
- groq/compound-mini
- meta-llama/llama-4-maverick-17b-128e-instruct
- meta-llama/llama-4-scout-17b-16e-instruct
- gemma2-9b-it
- mixtral-8x7b-32768

--------------------------------------------------------------------------------
ENDPOINT 4: Image Generation Proxy
--------------------------------------------------------------------------------

URL: POST /
Purpose: Proxy image generation to HuggingFace
Authentication: Handled server-side
Rate Limit: HuggingFace limits apply

Request Body:
{
  "type": "image",
  "model": "stabilityai/stable-diffusion-xl-base-1.0",
  "inputs": "A beautiful sunset over mountains"
}

Response: Binary image data (PNG)
Content-Type: image/png

Example cURL:
curl -X POST https://cheppu-aichatbox.onrender.com/ \
  -H "Content-Type: application/json" \
  -d '{
    "type": "image",
    "model": "stabilityai/stable-diffusion-xl-base-1.0",
    "inputs": "A beautiful sunset"
  }' \
  --output image.png

--------------------------------------------------------------------------------
ENDPOINT 5: Text-to-Speech Proxy
--------------------------------------------------------------------------------

URL: POST /
Purpose: Proxy TTS to HuggingFace
Authentication: Handled server-side
Rate Limit: HuggingFace limits apply

Request Body:
{
  "type": "audio",
  "model": "facebook/mms-tts-eng",
  "inputs": "Hello, this is a test."
}

Response: Binary audio data (WAV)
Content-Type: audio/wav

Example cURL:
curl -X POST https://cheppu-aichatbox.onrender.com/ \
  -H "Content-Type: application/json" \
  -d '{
    "type": "audio",
    "model": "facebook/mms-tts-eng",
    "inputs": "Hello world"
  }' \
  --output audio.wav

--------------------------------------------------------------------------------
                         ERROR RESPONSES
--------------------------------------------------------------------------------

400 Bad Request:
{
  "error": "Missing required fields: model and inputs"
}

401 Unauthorized (from Groq):
{
  "error": "Invalid API Key"
}

404 Not Found:
{
  "error": "Not found"
}

500 Internal Server Error:
{
  "error": "Internal server error",
  "message": "Network timeout"
}

503 Service Unavailable (HuggingFace):
{
  "error": "HuggingFace API error: 503",
  "details": "Model is currently loading"
}

================================================================================
                    8. REQUEST/RESPONSE EXAMPLES
================================================================================

--------------------------------------------------------------------------------
EXAMPLE 1: Simple Chat Request
--------------------------------------------------------------------------------

REQUEST:
POST https://cheppu-aichatbox.onrender.com/
Content-Type: application/json

{
  "type": "chat",
  "model": "llama-3.1-8b-instant",
  "messages": [
    {
      "role": "user",
      "content": "What is 2+2?"
    }
  ]
}

RESPONSE (200 OK):
{
  "id": "chatcmpl-xyz789",
  "object": "chat.completion",
  "created": 1732451234,
  "model": "llama-3.1-8b-instant",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "2 + 2 = 4"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 12,
    "completion_tokens": 5,
    "total_tokens": 17
  },
  "used_model": "llama-3.1-8b-instant"
}

--------------------------------------------------------------------------------
EXAMPLE 2: Chat with Model Fallback
--------------------------------------------------------------------------------

REQUEST:
POST https://cheppu-aichatbox.onrender.com/
Content-Type: application/json

{
  "type": "chat",
  "model": "invalid-model-name",
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    }
  ]
}

SERVER LOG:
[2025-11-24T13:00:00.000Z] Chat request. Attempting models: invalid-model-name, llama-3.1-8b-instant, ...
Trying model: invalid-model-name...
Model invalid-model-name failed: 404 - Model not found
Trying model: llama-3.1-8b-instant...
Success with model: llama-3.1-8b-instant

RESPONSE (200 OK):
{
  "id": "chatcmpl-abc456",
  "choices": [...],
  "used_model": "llama-3.1-8b-instant"  // Note: Different from requested
}

--------------------------------------------------------------------------------
EXAMPLE 3: Image Generation
--------------------------------------------------------------------------------

REQUEST:
POST https://cheppu-aichatbox.onrender.com/
Content-Type: application/json

{
  "type": "image",
  "model": "stabilityai/stable-diffusion-xl-base-1.0",
  "inputs": "A futuristic city with flying cars at sunset, 4k, detailed"
}

RESPONSE:
Content-Type: image/png
Content-Length: 2458392

[Binary PNG data]

--------------------------------------------------------------------------------
EXAMPLE 4: Health Check
--------------------------------------------------------------------------------

REQUEST:
GET https://cheppu-aichatbox.onrender.com/health

RESPONSE (200 OK):
{
  "status": "healthy",
  "timestamp": "2025-11-24T13:30:26.789Z"
}

================================================================================
                         9. ERROR HANDLING
================================================================================

ERROR HANDLING STRATEGY:
1. Model fallback for chat requests
2. Detailed error logging
3. User-friendly error messages
4. HTTP status code compliance
5. Graceful degradation

--------------------------------------------------------------------------------
                      CHAT REQUEST ERROR HANDLING
--------------------------------------------------------------------------------

Flow:
1. Try requested model
2. If fails (404, 503, etc.) â†’ Try next model in priority list
3. If 401 (auth error) â†’ Stop immediately, return error
4. If all models fail â†’ Return 500 with descriptive message

Error Priority:
1. Authentication errors (401) - Critical
2. Rate limit errors (429) - Retry with backoff
3. Model unavailable (503) - Try next model
4. Model not found (404) - Try next model
5. Network errors - Retry with timeout

Example Fallback Chain:
llama-3.3-70b-versatile (requested)
  â†“ (failed: 503 Model Loading)
llama-3.1-8b-instant
  â†“ (failed: 429 Rate Limit)
qwen/qwen3-32b
  âœ“ (success)

--------------------------------------------------------------------------------
                    IMAGE/TTS REQUEST ERROR HANDLING
--------------------------------------------------------------------------------

Common Errors:
1. Model loading (503) - Wait and retry
2. Invalid input (400) - Return to user
3. API key invalid (401) - Server misconfiguration
4. Rate limit (429) - Return to user with retry message

Error Message Format:
{
  "error": "HuggingFace API error: 503",
  "details": "Model is currently loading, estimated wait time: 20s"
}

--------------------------------------------------------------------------------
                      NETWORK ERROR HANDLING
--------------------------------------------------------------------------------

Timeout Settings:
- Request timeout: None (relies on Groq/HF limits)
- Connection timeout: 30 seconds
- Response timeout: 60 seconds

Retry Logic:
- No automatic retries (user can retry manually)
- Model fallback acts as implicit retry for chat

Error Categories:
1. ECONNREFUSED - Service down
2. ETIMEDOUT - Network timeout
3. ENOTFOUND - DNS resolution failed
4. ECONNRESET - Connection reset

================================================================================
                     10. DEPLOYMENT INFORMATION
================================================================================

PLATFORM: Render.com

SERVICE DETAILS:
----------------
Service ID: srv-abcdefghijklmnop
Service Name: cheppu-aichatbox
Service URL: https://cheppu-aichatbox.onrender.com
Dashboard: https://dashboard.render.com/

DEPLOYMENT SETTINGS:
--------------------
Branch: main
Auto-Deploy: Yes
Build Command: npm install
Start Command: node server.js
Health Check Path: /health
Health Check Interval: 30 seconds

DEPLOYMENT PROCESS:
-------------------
1. Developer pushes to GitHub main branch
2. Render detects new commit
3. Triggers build (npm install)
4. Runs start command (node server.js)
5. Health check passes
6. Traffic routed to new instance
7. Old instance terminated

Deployment Time: ~2-3 minutes

DEPLOYMENT HISTORY:
-------------------
Version 1.0: Initial deployment
Version 2.0: Added model fallback
Version 3.0: Groq API integration
Version 4.0: Multi-model support
Current: 7.0 (Production)

ROLLBACK PROCESS:
-----------------
1. Dashboard â†’ Deploys tab
2. Find previous successful deploy
3. Click "Redeploy"
4. Confirm rollback
5. Service reverts in ~2 minutes

MONITORING:
-----------
Logs: Real-time in Render dashboard
Metrics: CPU, Memory, Request count
Alerts: Email on service downtime
Status Page: https://status.render.com/

SCALING:
--------
Current: Free tier (512MB RAM, shared CPU)
Upgrade Options:
- Starter: $7/month (512MB, 0.1 CPU)
- Standard: $25/month (2GB, 0.5 CPU)
- Pro: $85/month (4GB, 1 CPU)

CUSTOM DOMAIN:
--------------
Supported: Yes
SSL: Automatic (Let's Encrypt)
Setup: Dashboard â†’ Settings â†’ Custom Domain

BACKUP & RESTORE:
-----------------
Code: Backed up in GitHub
Environment Variables: Export from dashboard
Database: N/A (stateless service)

================================================================================
                      11. MONITORING & LOGS
================================================================================

LOG LEVELS:
-----------
- console.log() - Info messages
- console.warn() - Warning messages
- console.error() - Error messages

LOGGED EVENTS:
--------------
1. Server startup
2. Environment variable status
3. Incoming request (timestamp, type, model)
4. Model attempt (success/failure)
5. API errors
6. Request completion (byte count)
7. Server shutdown

SAMPLE LOG OUTPUT:
------------------

Server starting...
PORT: 10000
HF_TOKEN present: true
API_KEY starts with: hf_abc...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ðŸš€ Cheppu AI Proxy Server                           â•‘
â•‘                                                        â•‘
â•‘   Running on: http://localhost:10000                  â•‘
â•‘   Environment: production                             â•‘
â•‘                                                        â•‘
â•‘   API Key: hf_abcdefg...                              â•‘
â•‘                                                        â•‘
â•‘   Ready to handle Image & TTS requests!               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[2025-11-24T13:00:00.123Z] Chat request. Attempting models: llama-3.1-8b-instant, llama-3.3-70b-versatile, qwen/qwen3-32b, groq/compound, groq/compound-mini, meta-llama/llama-4-maverick-17b-128e-instruct, meta-llama/llama-4-scout-17b-16e-instruct, gemma2-9b-it, mixtral-8x7b-32768
Trying model: llama-3.1-8b-instant...
Success with model: llama-3.1-8b-instant

[2025-11-24T13:00:05.456Z] image request for model: stabilityai/stable-diffusion-xl-base-1.0
Input length: 45 characters
Request completed successfully (2458392 bytes)

VIEWING LOGS:
-------------
Render Dashboard:
1. Go to service page
2. Click "Logs" tab
3. View real-time logs
4. Filter by keyword
5. Download logs

Command Line (Render CLI):
render logs cheppu-aichatbox --tail 100

ERROR TRACKING:
---------------
- Model failures logged with error codes
- API errors include full response text
- Network errors include error type
- All timestamps in ISO 8601 format

PERFORMANCE METRICS:
--------------------
Available in Render Dashboard:
- Request count
- Response time (avg, p50, p95, p99)
- CPU usage
- Memory usage
- Network bandwidth

CUSTOM MONITORING:
------------------
Can integrate with:
- Sentry (error tracking)
- LogRocket (session replay)
- DataDog (APM)
- New Relic (performance)

================================================================================
                     12. TROUBLESHOOTING GUIDE
================================================================================

ISSUE 1: Server Not Responding
--------------------------------------------------------------------------------
Symptoms: 504 Gateway Timeout, no response
Possible Causes:
1. Free tier auto-sleep (most common)
2. Service crash
3. Deployment in progress

Solutions:
1. Wait 30 seconds for cold start
2. Check Render dashboard for errors
3. Verify deployment status
4. Check server logs

Prevention:
- Ping server every 14 minutes to prevent sleep
- Upgrade to paid tier
- Use uptime monitor (uptimerobot.com)

--------------------------------------------------------------------------------
ISSUE 2: All Models Failing
--------------------------------------------------------------------------------
Symptoms: Error "All AI models are currently unavailable"
Possible Causes:
1. Invalid GROQ_API_KEY
2. Groq API rate limit exceeded
3. Groq service outage

Solutions:
1. Verify GROQ_API_KEY in environment variables
2. Check Groq API status: https://status.groq.com/
3. Wait for rate limit reset (1 minute)
4. Generate new API key

Prevention:
- Monitor API usage
- Implement request queuing
- Use multiple API keys (pro tier)

--------------------------------------------------------------------------------
ISSUE 3: Image Generation Failing
--------------------------------------------------------------------------------
Symptoms: HTTP 503, "Model is currently loading"
Possible Causes:
1. Model cold start (HuggingFace)
2. Invalid HF_TOKEN
3. Model unavailable

Solutions:
1. Wait 20-30 seconds and retry
2. Verify HF_TOKEN in environment
3. Check model status on HuggingFace
4. Try alternative model

Prevention:
- Use popular models (faster load)
- Implement retry with backoff
- Pre-warm models with test request

--------------------------------------------------------------------------------
ISSUE 4: CORS Errors
--------------------------------------------------------------------------------
Symptoms: "Access-Control-Allow-Origin" error in browser
Possible Causes:
1. CORS misconfiguration
2. Preflight request failed
3. Origin not allowed

Solutions:
1. Verify cors() middleware is enabled
2. Check headers in response
3. Test with cURL (bypasses CORS)

Prevention:
- Keep cors package up to date
- Test from multiple origins
- Monitor browser console

--------------------------------------------------------------------------------
ISSUE 5: Slow Response Times
--------------------------------------------------------------------------------
Symptoms: Requests taking >10 seconds
Possible Causes:
1. Cold start (free tier)
2. Model processing time
3. Network latency

Solutions:
1. Warm up server before use
2. Use faster models (8B instead of 70B)
3. Upgrade to paid tier

Prevention:
- Keep server warm
- Use CDN for static assets
- Implement caching

--------------------------------------------------------------------------------
ISSUE 6: Environment Variables Not Working
--------------------------------------------------------------------------------
Symptoms: "API_KEY starts with: YOUR_HU..."
Possible Causes:
1. Variables not set on Render
2. Service not restarted after change
3. Typo in variable name

Solutions:
1. Go to Render dashboard â†’ Environment
2. Verify HF_TOKEN and GROQ_API_KEY exist
3. Check for typos (case-sensitive)
4. Redeploy service

Prevention:
- Use .env.example as template
- Document all required variables
- Validate on startup

--------------------------------------------------------------------------------
ISSUE 7: Deployment Failing
--------------------------------------------------------------------------------
Symptoms: Build fails, service won't start
Possible Causes:
1. npm install fails
2. Syntax error in code
3. Missing dependencies

Solutions:
1. Check build logs in Render
2. Test locally: npm install && npm start
3. Verify package.json syntax
4. Clear build cache

Prevention:
- Test before pushing to main
- Use .nvmrc for Node version
- Lock dependency versions

--------------------------------------------------------------------------------
TROUBLESHOOTING CHECKLIST
--------------------------------------------------------------------------------

â–¡ Check Render service status (green = running)
â–¡ Verify environment variables are set
â–¡ Review recent deployment logs
â–¡ Test health endpoint: GET /health
â–¡ Check API key validity
â–¡ Monitor API rate limits
â–¡ Test locally with same environment
â–¡ Check for Groq/HuggingFace outages
â–¡ Verify CORS headers in response
â–¡ Check request payload format

SUPPORT RESOURCES:
------------------
Render Support: https://render.com/support
Groq Discord: https://discord.gg/groq
HuggingFace Forum: https://discuss.huggingface.co/
Project GitHub: [Your repository]

EMERGENCY CONTACTS:
-------------------
Platform Issues: Render support ticket
API Issues: Groq/HuggingFace support
Code Issues: Review GitHub issues

================================================================================
                           QUICK REFERENCE
================================================================================

BACKEND URL:
https://cheppu-aichatbox.onrender.com

HEALTH CHECK:
GET https://cheppu-aichatbox.onrender.com/health

CHAT REQUEST:
POST https://cheppu-aichatbox.onrender.com/
Body: { "type": "chat", "model": "llama-3.1-8b-instant", "messages": [...] }

IMAGE REQUEST:
POST https://cheppu-aichatbox.onrender.com/
Body: { "type": "image", "model": "stabilityai/...", "inputs": "..." }

ENVIRONMENT VARIABLES:
HF_TOKEN=hf_...
GROQ_API_KEY=gsk_...

LOGS:
Render Dashboard â†’ cheppu-aichatbox â†’ Logs

REDEPLOY:
git push origin main
(Auto-deploys to Render)

SUPPORTED MODELS:
Chat: llama-3.1-8b-instant, llama-3.3-70b-versatile, qwen/qwen3-32b, groq/compound, groq/compound-mini, meta-llama/llama-4-maverick-17b-128e-instruct, meta-llama/llama-4-scout-17b-16e-instruct, gemma2-9b-it, mixtral-8x7b-32768
Image: stabilityai/stable-diffusion-xl-base-1.0
TTS: facebook/mms-tts-eng

RATE LIMITS:
Groq: 30 RPM (Requests Per Minute)
HuggingFace: Varies by model

RESPONSE TIMES:
Chat: 1-3 seconds
Image: 5-15 seconds
TTS: 2-5 seconds

================================================================================
                              END OF DOCUMENT
================================================================================

Document Generated: November 24, 2025 19:00:26 IST
Backend Status: âœ… Production Live
Platform: Render.com
Version: 7.0

For updates, check the project repository or Render dashboard.

Questions? Review the troubleshooting section or contact support.

================================================================================
